{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Greater than 45' '25 - 45' 'Less than 25']\n",
      "['Male' 'Female']\n",
      "['Other' 'African-American' 'Caucasian' 'Hispanic' 'Native American'\n",
      " 'Asian']\n",
      "[ 0  4  1  2 14  3  7  6  5 13  8  9 21 20 15 10 12 28 19 11 22 23 25 24\n",
      " 36 18 16 33 17 30 27 38 26 37 29 35 31]\n",
      "['F' 'M']\n",
      "(7214, 14)\n",
      "Index(['sex', 'age_cat', 'race', 'juv_fel_count', 'juv_misd_count',\n",
      "       'juv_other_count', 'priors_count', 'c_charge_degree', 'decile_score',\n",
      "       'v_decile_score', 'score_text', 'v_score_text', 'is_recid',\n",
      "       'is_violent_recid'],\n",
      "      dtype='object')\n",
      "    sex          age_cat              race  juv_fel_count  juv_misd_count  \\\n",
      "0  Male  Greater than 45             Other              0               0   \n",
      "1  Male          25 - 45  African-American              0               0   \n",
      "2  Male     Less than 25  African-American              0               0   \n",
      "3  Male     Less than 25  African-American              0               1   \n",
      "4  Male          25 - 45             Other              0               0   \n",
      "\n",
      "   juv_other_count  priors_count c_charge_degree  decile_score  \\\n",
      "0                0             0               F             1   \n",
      "1                0             0               F             3   \n",
      "2                1             4               F             4   \n",
      "3                0             1               F             8   \n",
      "4                0             2               F             1   \n",
      "\n",
      "   v_decile_score score_text v_score_text  is_recid  is_violent_recid  \n",
      "0               1        Low          Low         0                 0  \n",
      "1               1        Low          Low         1                 1  \n",
      "2               3        Low          Low         1                 0  \n",
      "3               6       High       Medium         0                 0  \n",
      "4               1        Low          Low         0                 0  \n",
      "\n",
      "Shape after encoding: (7214, 20)\n",
      "\n",
      "New columns after encoding:\n",
      "Index(['juv_fel_count', 'juv_misd_count', 'juv_other_count', 'priors_count',\n",
      "       'decile_score', 'v_decile_score', 'score_text', 'v_score_text',\n",
      "       'is_recid', 'is_violent_recid', 'sex_Male', 'age_cat_Greater than 45',\n",
      "       'age_cat_Less than 25', 'c_charge_degree_M', 'race_African-American',\n",
      "       'race_Asian', 'race_Caucasian', 'race_Hispanic', 'race_Native American',\n",
      "       'race_Other'],\n",
      "      dtype='object')\n",
      "\n",
      "First few rows of encoded data:\n",
      "   juv_fel_count  juv_misd_count  juv_other_count  priors_count  decile_score  \\\n",
      "0              0               0                0             0             1   \n",
      "1              0               0                0             0             3   \n",
      "2              0               0                1             4             4   \n",
      "3              0               1                0             1             8   \n",
      "4              0               0                0             2             1   \n",
      "\n",
      "   v_decile_score  score_text  v_score_text  is_recid  is_violent_recid  \\\n",
      "0               1           0             0         0                 0   \n",
      "1               1           0             0         1                 1   \n",
      "2               3           0             0         1                 0   \n",
      "3               6           2             1         0                 0   \n",
      "4               1           0             0         0                 0   \n",
      "\n",
      "   sex_Male  age_cat_Greater than 45  age_cat_Less than 25  c_charge_degree_M  \\\n",
      "0         1                        1                     0                  0   \n",
      "1         1                        0                     0                  0   \n",
      "2         1                        0                     1                  0   \n",
      "3         1                        0                     1                  0   \n",
      "4         1                        0                     0                  0   \n",
      "\n",
      "   race_African-American  race_Asian  race_Caucasian  race_Hispanic  \\\n",
      "0                      0           0               0              0   \n",
      "1                      1           0               0              0   \n",
      "2                      1           0               0              0   \n",
      "3                      1           0               0              0   \n",
      "4                      0           0               0              0   \n",
      "\n",
      "   race_Native American  race_Other  \n",
      "0                     0           1  \n",
      "1                     0           0  \n",
      "2                     0           0  \n",
      "3                     0           0  \n",
      "4                     0           1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import classification_report, mean_squared_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('compas-scores-two-years.csv')\n",
    "\n",
    "print(df['age_cat'].unique())\n",
    "print(df['sex'].unique())\n",
    "print(df['race'].unique())\n",
    "print(df['priors_count'].unique())\n",
    "print(df['c_charge_degree'].unique())\n",
    "\n",
    "use_df = df[['sex','age_cat','race',\n",
    "            'juv_fel_count','juv_misd_count','juv_other_count','priors_count',\n",
    "            'c_charge_degree', 'decile_score', 'v_decile_score', 'score_text', 'v_score_text', 'is_recid', 'is_violent_recid']]\n",
    "\n",
    "print(use_df.shape)\n",
    "print(use_df.columns)\n",
    "print(use_df.head())\n",
    "# print(use_df['race'].unique())\n",
    "\n",
    "# Encode all categorical variables in one step\n",
    "categorical_columns = ['sex', 'age_cat', 'c_charge_degree'] #, 'score_text', 'v_score_text']\n",
    "encoded_df = pd.get_dummies(use_df, columns=categorical_columns, drop_first=True)\n",
    "\n",
    "# Now encode race separately without dropping any category\n",
    "encoded_df = pd.get_dummies(encoded_df, columns=['race'], drop_first=False)\n",
    "\n",
    "# Create mapping dictionary for score encoding\n",
    "score_mapping = {\n",
    "    'Low': 0,\n",
    "    'Medium': 1,\n",
    "    'High': 2\n",
    "}\n",
    "\n",
    "encoded_df['score_text'] = encoded_df['score_text'].map(score_mapping)\n",
    "encoded_df['v_score_text'] = encoded_df['v_score_text'].map(score_mapping)\n",
    "\n",
    "print(\"\\nShape after encoding:\", encoded_df.shape)\n",
    "print(\"\\nNew columns after encoding:\")\n",
    "print(encoded_df.columns)\n",
    "print(\"\\nFirst few rows of encoded data:\")\n",
    "print(encoded_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Regular Recidivism Prediction ===\n",
      "Training XGBoost for recidivism prediction...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "Best parameters found:\n",
      "{'colsample_bytree': 1.0, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'objective': 'binary:logistic', 'subsample': 1.0}\n",
      "\n",
      "Best cross-validation score: 0.6483384017009722\n",
      "\n",
      "Classification Report for Recidivism Prediction:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.72      0.71      1153\n",
      "           1       0.67      0.66      0.67      1012\n",
      "\n",
      "    accuracy                           0.69      2165\n",
      "   macro avg       0.69      0.69      0.69      2165\n",
      "weighted avg       0.69      0.69      0.69      2165\n",
      "\n",
      "\n",
      "Top 10 Most Important Features for Recidivism:\n",
      "                    feature  importance\n",
      "6      age_cat_Less than 25    0.273978\n",
      "3              priors_count    0.226027\n",
      "5   age_cat_Greater than 45    0.149772\n",
      "4                  sex_Male    0.076848\n",
      "8     race_African-American    0.048554\n",
      "2           juv_other_count    0.041339\n",
      "7         c_charge_degree_M    0.032224\n",
      "0             juv_fel_count    0.029071\n",
      "10           race_Caucasian    0.028747\n",
      "11            race_Hispanic    0.026289\n",
      "\n",
      "\n",
      "=== Violent Recidivism Prediction ===\n",
      "Training XGBoost for violent recidivism prediction...\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "\n",
      "Best parameters found:\n",
      "{'colsample_bytree': 1.0, 'gamma': 0.1, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 1, 'n_estimators': 200, 'objective': 'binary:logistic', 'subsample': 1.0}\n",
      "\n",
      "Best cross-validation score: 0.016697040333403967\n",
      "\n",
      "Classification Report for Violent Recidivism Prediction:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94      1925\n",
      "           1       0.25      0.01      0.02       240\n",
      "\n",
      "    accuracy                           0.89      2165\n",
      "   macro avg       0.57      0.50      0.48      2165\n",
      "weighted avg       0.82      0.89      0.84      2165\n",
      "\n",
      "\n",
      "Top 10 Most Important Features for Violent Recidivism:\n",
      "                    feature  importance\n",
      "5   age_cat_Greater than 45    0.213258\n",
      "6      age_cat_Less than 25    0.113338\n",
      "3              priors_count    0.091019\n",
      "4                  sex_Male    0.088517\n",
      "7         c_charge_degree_M    0.076386\n",
      "13               race_Other    0.072216\n",
      "10           race_Caucasian    0.067532\n",
      "11            race_Hispanic    0.054053\n",
      "1            juv_misd_count    0.050300\n",
      "2           juv_other_count    0.045557\n"
     ]
    }
   ],
   "source": [
    "# Split the data into features and target variables\n",
    "X = encoded_df.drop(columns=['decile_score', 'v_decile_score', 'score_text', 'v_score_text', 'is_recid', 'is_violent_recid'])\n",
    "class_y = encoded_df['is_recid']  # Binary target (0 or 1)\n",
    "v_class_y = encoded_df['is_violent_recid']\n",
    "\n",
    "# Create label encoders\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(class_y)\n",
    "v_y_encoded = le.fit_transform(v_class_y)\n",
    "\n",
    "# Split into train and test sets - do this separately for each target\n",
    "X_train, X_test, y_train_decile, y_test_decile = train_test_split(\n",
    "    X, y_encoded, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# For violent recidivism, use the same split indices\n",
    "_, _, v_y_train, v_y_test = train_test_split(\n",
    "    X, v_y_encoded, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3],              \n",
    "    'learning_rate': [0.1],   \n",
    "    'n_estimators': [200],         \n",
    "    'min_child_weight': [1],\n",
    "    'gamma': [0.1],\n",
    "    'subsample': [1.0],\n",
    "    'colsample_bytree': [1.0],\n",
    "    'objective': ['binary:logistic']\n",
    "}\n",
    "\n",
    "# Train and evaluate for regular recidivism\n",
    "print(\"\\n=== Regular Recidivism Prediction ===\")\n",
    "xgb_recid = XGBClassifier(random_state=42, objective='binary:logistic')\n",
    "grid_search_recid = GridSearchCV(\n",
    "    estimator=xgb_recid,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"Training XGBoost for recidivism prediction...\")\n",
    "grid_search_recid.fit(X_train, y_train_decile)\n",
    "\n",
    "print(\"\\nBest parameters found:\")\n",
    "print(grid_search_recid.best_params_)\n",
    "print(\"\\nBest cross-validation score:\", grid_search_recid.best_score_)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred_recid = grid_search_recid.best_estimator_.predict(X_test)\n",
    "print(\"\\nClassification Report for Recidivism Prediction:\")\n",
    "print(classification_report(y_test_decile, y_pred_recid))\n",
    "\n",
    "# Feature importance for recidivism\n",
    "feature_importance_recid = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': grid_search_recid.best_estimator_.feature_importances_\n",
    "})\n",
    "feature_importance_recid = feature_importance_recid.sort_values('importance', ascending=False)\n",
    "print(\"\\nTop 10 Most Important Features for Recidivism:\")\n",
    "print(feature_importance_recid.head(10))\n",
    "\n",
    "# Train and evaluate for violent recidivism\n",
    "print(\"\\n\\n=== Violent Recidivism Prediction ===\")\n",
    "xgb_violent = XGBClassifier(random_state=42, objective='binary:logistic')\n",
    "grid_search_violent = GridSearchCV(\n",
    "    estimator=xgb_violent,\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1',\n",
    "    n_jobs=-1,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "print(\"Training XGBoost for violent recidivism prediction...\")\n",
    "grid_search_violent.fit(X_train, v_y_train)\n",
    "\n",
    "print(\"\\nBest parameters found:\")\n",
    "print(grid_search_violent.best_params_)\n",
    "print(\"\\nBest cross-validation score:\", grid_search_violent.best_score_)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred_violent = grid_search_violent.best_estimator_.predict(X_test)\n",
    "print(\"\\nClassification Report for Violent Recidivism Prediction:\")\n",
    "print(classification_report(v_y_test, y_pred_violent))\n",
    "\n",
    "# Feature importance for violent recidivism\n",
    "feature_importance_violent = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': grid_search_violent.best_estimator_.feature_importances_\n",
    "})\n",
    "feature_importance_violent = feature_importance_violent.sort_values('importance', ascending=False)\n",
    "print(\"\\nTop 10 Most Important Features for Violent Recidivism:\")\n",
    "print(feature_importance_violent.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fairness Analysis ===\n",
      "\n",
      "Race Fairness Metrics:\n",
      "Original dataset metrics:\n",
      "Disparate Impact: 1.3873652566225543\n",
      "Statistical Parity Difference: 0.14814727582639403\n",
      "\n",
      "Prediction metrics:\n",
      "Equal Opportunity Difference: 0.2767348588083069\n",
      "Average Odds Difference: 0.23412289869487596\n",
      "Disparate Impact: 1.9552412190788575\n",
      "Statistical Parity Difference: 0.2804662734240199\n",
      "Equalized Odds Difference: 0.2767348588083069\n",
      "===== Privileged Group Metrics =====\n",
      "TPR:  0.47592067988668557\n",
      "FPR:  0.18070175438596492\n",
      "TNR:  0.8192982456140351\n",
      "FNR:  0.5240793201133145\n",
      "PPV:  0.6199261992619927\n",
      "===== Unprivileged Group Metrics =====\n",
      "TPR:  0.7526555386949925\n",
      "FPR:  0.37221269296740994\n",
      "TNR:  0.62778730703259\n",
      "FNR:  0.2473444613050076\n",
      "PPV:  0.6956521739130435\n",
      "\n",
      "Sex Fairness Metrics:\n",
      "Original dataset metrics:\n",
      "Disparate Impact: 1.3426392572944297\n",
      "Statistical Parity Difference: 0.1251453206742879\n",
      "\n",
      "Prediction metrics:\n",
      "Equal Opportunity Difference: 0.298961937716263\n",
      "Average Odds Difference: 0.23057692925486883\n",
      "Disparate Impact: 2.077064479638009\n",
      "Statistical Parity Difference: 0.26044884142380065\n",
      "Equalized Odds Difference: 0.298961937716263\n",
      "===== Privileged Group Metrics =====\n",
      "TPR:  0.4\n",
      "FPR:  0.15079365079365079\n",
      "TNR:  0.8492063492063492\n",
      "FNR:  0.6\n",
      "PPV:  0.6041666666666666\n",
      "===== Unprivileged Group Metrics =====\n",
      "TPR:  0.698961937716263\n",
      "FPR:  0.3129855715871254\n",
      "TNR:  0.6870144284128746\n",
      "FNR:  0.30103806228373703\n",
      "PPV:  0.6824324324324325\n",
      "\n",
      "Age Fairness Metrics:\n",
      "Original dataset metrics:\n",
      "Disparate Impact: 2.0086876720709697\n",
      "Statistical Parity Difference: 0.3096848116007363\n",
      "\n",
      "Prediction metrics:\n",
      "Equal Opportunity Difference: 0.30307539682539686\n",
      "Average Odds Difference: 0.3516561483101597\n",
      "Disparate Impact: 2.866048060908875\n",
      "Statistical Parity Difference: 0.4419587512678914\n",
      "Equalized Odds Difference: 0.40023689979492255\n",
      "===== Privileged Group Metrics =====\n",
      "TPR:  0.4642857142857143\n",
      "FPR:  0.1360759493670886\n",
      "TNR:  0.8639240506329114\n",
      "FNR:  0.5357142857142857\n",
      "PPV:  0.6018518518518519\n",
      "===== Unprivileged Group Metrics =====\n",
      "TPR:  0.7673611111111112\n",
      "FPR:  0.5363128491620112\n",
      "TNR:  0.46368715083798884\n",
      "FNR:  0.2326388888888889\n",
      "PPV:  0.6971608832807571\n"
     ]
    }
   ],
   "source": [
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.metrics import ClassificationMetric\n",
    "import numpy as np\n",
    "\n",
    "# First, create functions to help with the analysis\n",
    "def create_aif_dataset(X, y, protected_attribute_names, favorable_label=1, unfavorable_label=0):\n",
    "    \"\"\"\n",
    "    Create an AIF360 BinaryLabelDataset from features and labels\n",
    "    \"\"\"\n",
    "    df = pd.concat([X, pd.Series(y, index=X.index)], axis=1)\n",
    "    return BinaryLabelDataset(\n",
    "        df=df,\n",
    "        label_names=[df.columns[-1]],  # last column is the label\n",
    "        protected_attribute_names=protected_attribute_names,\n",
    "        favorable_label=favorable_label,\n",
    "        unfavorable_label=unfavorable_label\n",
    "    )\n",
    "\n",
    "def compute_fairness_metrics(dataset, privileged_groups, unprivileged_groups):\n",
    "    \"\"\"\n",
    "    Compute fairness metrics for a dataset\n",
    "    \"\"\"\n",
    "    metrics = BinaryLabelDatasetMetric(\n",
    "        dataset,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups\n",
    "    )\n",
    "    \n",
    "    print(\"Disparate Impact:\", metrics.disparate_impact())\n",
    "    print(\"Statistical Parity Difference:\", metrics.statistical_parity_difference())\n",
    "    # print(\"Equalized Odds Difference:\", metrics.equalized_odds_difference())\n",
    "    # print(\"Equal Opportunity Difference:\", metrics.equal_opportunity_difference())\n",
    "    \n",
    "def compute_classification_metrics(dataset_true, dataset_pred, privileged_groups, unprivileged_groups):\n",
    "    \"\"\"\n",
    "    Compute classification fairness metrics\n",
    "    \"\"\"\n",
    "    metrics = ClassificationMetric(\n",
    "        dataset_true,\n",
    "        dataset_pred,\n",
    "        unprivileged_groups=unprivileged_groups,\n",
    "        privileged_groups=privileged_groups\n",
    "    )\n",
    "    \n",
    "    print(\"Equal Opportunity Difference:\", metrics.equal_opportunity_difference())\n",
    "    print(\"Average Odds Difference:\", metrics.average_odds_difference())\n",
    "    print(\"Disparate Impact:\", metrics.disparate_impact())\n",
    "    print(\"Statistical Parity Difference:\", metrics.statistical_parity_difference())\n",
    "    print(\"Equalized Odds Difference:\", metrics.equalized_odds_difference())\n",
    "    # print(\"Positive Predictive Value:\", metrics.positive_predictive_value())\n",
    "    # print(\"True Positive Rate Difference:\", metrics.true_positive_rate_difference())\n",
    "    # print(\"False Positive Rate Difference:\", metrics.false_positive_rate_difference())\n",
    "    print(\"===== Privileged Group Metrics =====\")\n",
    "    print(\"TPR: \", metrics.true_positive_rate(privileged=True))\n",
    "    print(\"FPR: \", metrics.false_positive_rate(privileged=True))\n",
    "    print(\"TNR: \", metrics.true_negative_rate(privileged=True))\n",
    "    print(\"FNR: \", metrics.false_negative_rate(privileged=True))\n",
    "    print(\"PPV: \", metrics.positive_predictive_value(privileged=True))\n",
    "    print(\"===== Unprivileged Group Metrics =====\")\n",
    "    print(\"TPR: \", metrics.true_positive_rate(privileged=False))\n",
    "    print(\"FPR: \", metrics.false_positive_rate(privileged=False))\n",
    "    print(\"TNR: \", metrics.true_negative_rate(privileged=False))\n",
    "    print(\"FNR: \", metrics.false_negative_rate(privileged=False))\n",
    "    print(\"PPV: \", metrics.positive_predictive_value(privileged=False))\n",
    "\n",
    "# Analyze fairness for different protected attributes\n",
    "print(\"\\n=== Fairness Analysis ===\")\n",
    "\n",
    "# 1. Race Analysis\n",
    "protected_attribute_names_race = ['race_African-American', 'race_Asian', 'race_Caucasian', 'race_Hispanic', 'race_Native American', 'race_Other']\n",
    "privileged_groups_race = [{'race_Caucasian': 1}, {'race_Hispanic': 1}]\n",
    "unprivileged_groups_race = [\n",
    "    {'race_African-American': 1},\n",
    "    {'race_Asian': 1},\n",
    "    # {'race_Hispanic': 1},\n",
    "    {'race_Native American': 1},\n",
    "    {'race_Other': 1}\n",
    "]\n",
    "\n",
    "# Create datasets for true values and predictions\n",
    "dataset_true = create_aif_dataset(X_test, y_test_decile, protected_attribute_names_race)\n",
    "dataset_pred = create_aif_dataset(X_test, y_pred_recid, protected_attribute_names_race)\n",
    "\n",
    "print(\"\\nRace Fairness Metrics:\")\n",
    "print(\"Original dataset metrics:\")\n",
    "compute_fairness_metrics(dataset_true, privileged_groups_race, unprivileged_groups_race)\n",
    "print(\"\\nPrediction metrics:\")\n",
    "compute_classification_metrics(dataset_true, dataset_pred, privileged_groups_race, unprivileged_groups_race)\n",
    "\n",
    "# 2. Sex Analysis\n",
    "protected_attribute_names_sex = ['sex_Male']\n",
    "privileged_groups_sex = [{'sex_Male': 0}]\n",
    "unprivileged_groups_sex = [{'sex_Male': 1}]\n",
    "\n",
    "dataset_true = create_aif_dataset(X_test, y_test_decile, protected_attribute_names_sex)\n",
    "dataset_pred = create_aif_dataset(X_test, y_pred_recid, protected_attribute_names_sex)\n",
    "\n",
    "print(\"\\nSex Fairness Metrics:\")\n",
    "print(\"Original dataset metrics:\")\n",
    "compute_fairness_metrics(dataset_true, privileged_groups_sex, unprivileged_groups_sex)\n",
    "print(\"\\nPrediction metrics:\")\n",
    "compute_classification_metrics(dataset_true, dataset_pred, privileged_groups_sex, unprivileged_groups_sex)\n",
    "\n",
    "# 3. Age Analysis\n",
    "protected_attribute_names_age = ['age_cat_Greater than 45', 'age_cat_Less than 25']\n",
    "privileged_groups_age = [{'age_cat_Greater than 45': 1}]\n",
    "unprivileged_groups_age = [\n",
    "    {'age_cat_Less than 25': 1}, \n",
    "    # {'age_cat_Greater than 45': 0, 'age_cat_Less than 25': 0}  # Group for 25-45 years\n",
    "]\n",
    "\n",
    "# Create datasets for true values and predictions\n",
    "dataset_true = create_aif_dataset(X_test, y_test_decile, protected_attribute_names_age)\n",
    "dataset_pred = create_aif_dataset(X_test, y_pred_recid, protected_attribute_names_age)\n",
    "\n",
    "print(\"\\nAge Fairness Metrics:\")\n",
    "print(\"Original dataset metrics:\")\n",
    "compute_fairness_metrics(dataset_true, privileged_groups_age, unprivileged_groups_age)\n",
    "print(\"\\nPrediction metrics:\")\n",
    "compute_classification_metrics(dataset_true, dataset_pred, privileged_groups_age, unprivileged_groups_age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Base Model ===\n",
      "\n",
      "=== Base Model Training and Evaluation ===\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.72      0.71      1153\n",
      "           1       0.67      0.66      0.67      1012\n",
      "\n",
      "    accuracy                           0.69      2165\n",
      "   macro avg       0.69      0.69      0.69      2165\n",
      "weighted avg       0.69      0.69      0.69      2165\n",
      "\n",
      "\n",
      "=== Fairness Metrics for Base Model ===\n",
      "\n",
      "Race Fairness Metrics:\n",
      "Equal Opportunity Difference: 0.2725040916530278\n",
      "Average Odds Difference: 0.2475651339360092\n",
      "Disparate Impact: 1.9816801426115747\n",
      "Statistical Parity Difference: 0.2998104219327241\n",
      "Equalized Odds Difference: 0.2725040916530278\n",
      "===== Privileged Group Metrics =====\n",
      "TPR:  0.5\n",
      "FPR:  0.17857142857142858\n",
      "TNR:  0.8214285714285714\n",
      "FNR:  0.5\n",
      "PPV:  0.6460176991150443\n",
      "===== Unprivileged Group Metrics =====\n",
      "TPR:  0.7725040916530278\n",
      "FPR:  0.40119760479041916\n",
      "TNR:  0.5988023952095808\n",
      "FNR:  0.22749590834697217\n",
      "PPV:  0.7013372956909361\n",
      "\n",
      "Sex Fairness Metrics:\n",
      "Equal Opportunity Difference: 0.298961937716263\n",
      "Average Odds Difference: 0.23057692925486883\n",
      "Disparate Impact: 2.077064479638009\n",
      "Statistical Parity Difference: 0.26044884142380065\n",
      "Equalized Odds Difference: 0.298961937716263\n",
      "===== Privileged Group Metrics =====\n",
      "TPR:  0.4\n",
      "FPR:  0.15079365079365079\n",
      "TNR:  0.8492063492063492\n",
      "FNR:  0.6\n",
      "PPV:  0.6041666666666666\n",
      "===== Unprivileged Group Metrics =====\n",
      "TPR:  0.698961937716263\n",
      "FPR:  0.3129855715871254\n",
      "TNR:  0.6870144284128746\n",
      "FNR:  0.30103806228373703\n",
      "PPV:  0.6824324324324325\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "\n",
    "def train_and_evaluate_model(X_train, X_test, y_train, y_test, model_name=\"Base Model\"):\n",
    "    \"\"\"\n",
    "    Train XGBoost model and print evaluation metrics\n",
    "    \"\"\"\n",
    "    param_grid = {\n",
    "        'max_depth': [3],              \n",
    "        'learning_rate': [0.1],   \n",
    "        'n_estimators': [200],         \n",
    "        'min_child_weight': [1],\n",
    "        'gamma': [0.1],\n",
    "        'subsample': [1.0],\n",
    "        'colsample_bytree': [1.0],\n",
    "        'objective': ['binary:logistic']\n",
    "    }\n",
    "    \n",
    "    xgb_model = XGBClassifier(random_state=42, objective='binary:logistic')\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring='f1',\n",
    "        n_jobs=-1,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n=== {model_name} Training and Evaluation ===\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    y_pred = grid_search.predict(X_test)\n",
    "    \n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    return grid_search, y_pred\n",
    "\n",
    "def evaluate_fairness(X_test, y_test, y_pred, model_name=\"Base Model\"):\n",
    "    \"\"\"\n",
    "    Evaluate fairness metrics for different protected attributes\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Fairness Metrics for {model_name} ===\")\n",
    "    \n",
    "    # Race Analysis\n",
    "    protected_attribute_names_race = ['race_African-American', 'race_Caucasian']\n",
    "    privileged_groups_race = [{'race_Caucasian': 1}]\n",
    "    unprivileged_groups_race = [{'race_African-American': 1}]\n",
    "    \n",
    "    dataset_true = create_aif_dataset(X_test, y_test, protected_attribute_names_race)\n",
    "    dataset_pred = create_aif_dataset(X_test, y_pred, protected_attribute_names_race)\n",
    "    \n",
    "    print(\"\\nRace Fairness Metrics:\")\n",
    "    compute_classification_metrics(dataset_true, dataset_pred, \n",
    "                                privileged_groups_race, \n",
    "                                unprivileged_groups_race)\n",
    "    \n",
    "    # Sex Analysis\n",
    "    protected_attribute_names_sex = ['sex_Male']\n",
    "    privileged_groups_sex = [{'sex_Male': 0}]\n",
    "    unprivileged_groups_sex = [{'sex_Male': 1}]\n",
    "    \n",
    "    dataset_true = create_aif_dataset(X_test, y_test, protected_attribute_names_sex)\n",
    "    dataset_pred = create_aif_dataset(X_test, y_pred, protected_attribute_names_sex)\n",
    "    \n",
    "    print(\"\\nSex Fairness Metrics:\")\n",
    "    compute_classification_metrics(dataset_true, dataset_pred, \n",
    "                                privileged_groups_sex, \n",
    "                                unprivileged_groups_sex)\n",
    "\n",
    "def balanced_resample_by_group(X, y, protected_attribute):\n",
    "    \"\"\"\n",
    "    Performs balanced resampling within each protected group\n",
    "    \"\"\"\n",
    "    resampled_X = pd.DataFrame()\n",
    "    resampled_y = pd.Series(dtype='float64')  # Specify dtype to avoid warning\n",
    "    \n",
    "    groups = X[protected_attribute].unique()\n",
    "    \n",
    "    for group in groups:\n",
    "        mask = X[protected_attribute] == group\n",
    "        X_group = X[mask]\n",
    "        y_group = y[mask]\n",
    "        \n",
    "        oversample = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "        undersample = RandomUnderSampler(sampling_strategy='auto', random_state=42)\n",
    "        \n",
    "        steps = [('oversample', oversample), ('undersample', undersample)]\n",
    "        pipeline = Pipeline(steps=steps)\n",
    "        \n",
    "        X_res, y_res = pipeline.fit_resample(X_group, y_group)\n",
    "        \n",
    "        resampled_X = pd.concat([resampled_X, pd.DataFrame(X_res, columns=X.columns)])\n",
    "        resampled_y = pd.concat([resampled_y, pd.Series(y_res)])\n",
    "    \n",
    "    return resampled_X, resampled_y\n",
    "\n",
    "\n",
    "# Train and evaluate base model\n",
    "print(\"\\n=== Base Model ===\")\n",
    "base_model, base_predictions = train_and_evaluate_model(\n",
    "    X_train, X_test, y_train_decile, y_test_decile, \"Base Model\"\n",
    ")\n",
    "evaluate_fairness(X_test, y_test_decile, base_predictions, \"Base Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Race-Based Resampling Model ===\n",
      "\n",
      "=== Race-Resampled Model Training and Evaluation ===\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.69      0.69      1153\n",
      "           1       0.65      0.66      0.66      1012\n",
      "\n",
      "    accuracy                           0.67      2165\n",
      "   macro avg       0.67      0.67      0.67      2165\n",
      "weighted avg       0.67      0.67      0.67      2165\n",
      "\n",
      "\n",
      "=== Fairness Metrics for Race-Resampled Model ===\n",
      "\n",
      "Race Fairness Metrics:\n",
      "Equal Opportunity Difference: 0.026892809900679326\n",
      "Average Odds Difference: -0.0022677025489474534\n",
      "Disparate Impact: 1.1078078713499788\n",
      "Statistical Parity Difference: 0.049533346295936254\n",
      "Equalized Odds Difference: 0.03142821499857423\n",
      "===== Privileged Group Metrics =====\n",
      "TPR:  0.6506849315068494\n",
      "FPR:  0.33482142857142855\n",
      "TNR:  0.6651785714285714\n",
      "FNR:  0.3493150684931507\n",
      "PPV:  0.5588235294117647\n",
      "===== Unprivileged Group Metrics =====\n",
      "TPR:  0.6775777414075287\n",
      "FPR:  0.3033932135728543\n",
      "TNR:  0.6966067864271457\n",
      "FNR:  0.32242225859247137\n",
      "PPV:  0.7314487632508834\n",
      "\n",
      "Sex Fairness Metrics:\n",
      "Equal Opportunity Difference: 0.346132124249294\n",
      "Average Odds Difference: 0.2704071998375938\n",
      "Disparate Impact: 2.24308630981034\n",
      "Statistical Parity Difference: 0.2943327786452694\n",
      "Equalized Odds Difference: 0.346132124249294\n",
      "===== Privileged Group Metrics =====\n",
      "TPR:  0.36551724137931035\n",
      "FPR:  0.1626984126984127\n",
      "TNR:  0.8373015873015873\n",
      "FNR:  0.6344827586206897\n",
      "PPV:  0.5638297872340425\n",
      "===== Unprivileged Group Metrics =====\n",
      "TPR:  0.7116493656286044\n",
      "FPR:  0.35738068812430634\n",
      "TNR:  0.6426193118756937\n",
      "FNR:  0.28835063437139563\n",
      "PPV:  0.6570820021299254\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate model with race-based resampling\n",
    "print(\"\\n=== Race-Based Resampling Model ===\")\n",
    "X_train_resampled_race, y_train_resampled_race = balanced_resample_by_group(\n",
    "    X_train, y_train_decile, 'race_African-American'\n",
    ")\n",
    "race_model, race_predictions = train_and_evaluate_model(\n",
    "    X_train_resampled_race, X_test, y_train_resampled_race, y_test_decile,\n",
    "    \"Race-Resampled Model\"\n",
    ")\n",
    "evaluate_fairness(X_test, y_test_decile, race_predictions, \"Race-Resampled Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Age-Based Resampling Model ===\n",
      "\n",
      "=== Age-Resampled Model Training and Evaluation ===\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.72      0.71      1153\n",
      "           1       0.66      0.64      0.65      1012\n",
      "\n",
      "    accuracy                           0.68      2165\n",
      "   macro avg       0.68      0.68      0.68      2165\n",
      "weighted avg       0.68      0.68      0.68      2165\n",
      "\n",
      "\n",
      "=== Fairness Metrics for Age-Resampled Model ===\n",
      "\n",
      "Race Fairness Metrics:\n",
      "Equal Opportunity Difference: 0.1955529897092123\n",
      "Average Odds Difference: 0.17455597161537034\n",
      "Disparate Impact: 1.6418293936279549\n",
      "Statistical Parity Difference: 0.22464028776978417\n",
      "Equalized Odds Difference: 0.1955529897092123\n",
      "===== Privileged Group Metrics =====\n",
      "TPR:  0.5376712328767124\n",
      "FPR:  0.22767857142857142\n",
      "TNR:  0.7723214285714286\n",
      "FNR:  0.4623287671232877\n",
      "PPV:  0.6061776061776062\n",
      "===== Unprivileged Group Metrics =====\n",
      "TPR:  0.7332242225859247\n",
      "FPR:  0.3812375249500998\n",
      "TNR:  0.6187624750499002\n",
      "FNR:  0.26677577741407527\n",
      "PPV:  0.701095461658842\n",
      "\n",
      "Sex Fairness Metrics:\n",
      "Equal Opportunity Difference: 0.2816608996539792\n",
      "Average Odds Difference: 0.21565471915736328\n",
      "Disparate Impact: 1.9670361990950227\n",
      "Statistical Parity Difference: 0.24358594435642889\n",
      "Equalized Odds Difference: 0.2816608996539792\n",
      "===== Privileged Group Metrics =====\n",
      "TPR:  0.4\n",
      "FPR:  0.16666666666666666\n",
      "TNR:  0.8333333333333334\n",
      "FNR:  0.6\n",
      "PPV:  0.58\n",
      "===== Unprivileged Group Metrics =====\n",
      "TPR:  0.6816608996539792\n",
      "FPR:  0.316315205327414\n",
      "TNR:  0.683684794672586\n",
      "FNR:  0.31833910034602075\n",
      "PPV:  0.6746575342465754\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate model with age-based resampling\n",
    "print(\"\\n=== Age-Based Resampling Model ===\")\n",
    "X_train_resampled_age, y_train_resampled_age = balanced_resample_by_group(\n",
    "    X_train, y_train_decile, 'age_cat_Greater than 45'\n",
    ")\n",
    "age_model, age_predictions = train_and_evaluate_model(\n",
    "    X_train_resampled_age, X_test, y_train_resampled_age, y_test_decile,\n",
    "    \"Age-Resampled Model\"\n",
    ")\n",
    "evaluate_fairness(X_test, y_test_decile, age_predictions, \"Age-Resampled Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sex-Based Resampling Model ===\n",
      "\n",
      "=== Sex-Resampled Model Training and Evaluation ===\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.69      0.69      1153\n",
      "           1       0.65      0.67      0.66      1012\n",
      "\n",
      "    accuracy                           0.68      2165\n",
      "   macro avg       0.68      0.68      0.68      2165\n",
      "weighted avg       0.68      0.68      0.68      2165\n",
      "\n",
      "\n",
      "=== Fairness Metrics for Sex-Resampled Model ===\n",
      "\n",
      "Race Fairness Metrics:\n",
      "Equal Opportunity Difference: 0.24807748357733783\n",
      "Average Odds Difference: 0.22976033431789616\n",
      "Disparate Impact: 1.8003736545647202\n",
      "Statistical Parity Difference: 0.27904919307797005\n",
      "Equalized Odds Difference: 0.24807748357733783\n",
      "===== Privileged Group Metrics =====\n",
      "TPR:  0.5342465753424658\n",
      "FPR:  0.22767857142857142\n",
      "TNR:  0.7723214285714286\n",
      "FNR:  0.4657534246575342\n",
      "PPV:  0.6046511627906976\n",
      "===== Unprivileged Group Metrics =====\n",
      "TPR:  0.7823240589198036\n",
      "FPR:  0.43912175648702595\n",
      "TNR:  0.5608782435129741\n",
      "FNR:  0.2176759410801964\n",
      "PPV:  0.6848137535816619\n",
      "\n",
      "Sex Fairness Metrics:\n",
      "Equal Opportunity Difference: 0.19782046692916522\n",
      "Average Odds Difference: 0.160704448005745\n",
      "Disparate Impact: 1.6069181278280544\n",
      "Statistical Parity Difference: 0.19568141149116114\n",
      "Equalized Odds Difference: 0.19782046692916522\n",
      "===== Privileged Group Metrics =====\n",
      "TPR:  0.503448275862069\n",
      "FPR:  0.21825396825396826\n",
      "TNR:  0.7817460317460317\n",
      "FNR:  0.496551724137931\n",
      "PPV:  0.5703125\n",
      "===== Unprivileged Group Metrics =====\n",
      "TPR:  0.7012687427912342\n",
      "FPR:  0.341842397336293\n",
      "TNR:  0.658157602663707\n",
      "FNR:  0.29873125720876587\n",
      "PPV:  0.6637554585152838\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate model with sex-based resampling\n",
    "print(\"\\n=== Sex-Based Resampling Model ===\")\n",
    "X_train_resampled_sex, y_train_resampled_sex = balanced_resample_by_group(\n",
    "    X_train, y_train_decile, 'sex_Male'\n",
    ")\n",
    "sex_model, sex_predictions = train_and_evaluate_model(\n",
    "    X_train_resampled_sex, X_test, y_train_resampled_sex, y_test_decile,\n",
    "    \"Sex-Resampled Model\"\n",
    ")\n",
    "evaluate_fairness(X_test, y_test_decile, sex_predictions, \"Sex-Resampled Model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sex-Based Resampling Model (Female) ===\n",
      "\n",
      "=== Sex-Resampled Model (Female) Training and Evaluation ===\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.69      0.69      1153\n",
      "           1       0.65      0.67      0.66      1012\n",
      "\n",
      "    accuracy                           0.68      2165\n",
      "   macro avg       0.68      0.68      0.68      2165\n",
      "weighted avg       0.68      0.68      0.68      2165\n",
      "\n",
      "\n",
      "=== Fairness Metrics for Sex-Resampled Model (Female) ===\n",
      "\n",
      "Race Fairness Metrics:\n",
      "Equal Opportunity Difference: 0.24807748357733783\n",
      "Average Odds Difference: 0.22976033431789616\n",
      "Disparate Impact: 1.8003736545647202\n",
      "Statistical Parity Difference: 0.27904919307797005\n",
      "Equalized Odds Difference: 0.24807748357733783\n",
      "===== Privileged Group Metrics =====\n",
      "TPR:  0.5342465753424658\n",
      "FPR:  0.22767857142857142\n",
      "TNR:  0.7723214285714286\n",
      "FNR:  0.4657534246575342\n",
      "PPV:  0.6046511627906976\n",
      "===== Unprivileged Group Metrics =====\n",
      "TPR:  0.7823240589198036\n",
      "FPR:  0.43912175648702595\n",
      "TNR:  0.5608782435129741\n",
      "FNR:  0.2176759410801964\n",
      "PPV:  0.6848137535816619\n",
      "\n",
      "Sex Fairness Metrics:\n",
      "Equal Opportunity Difference: 0.19782046692916522\n",
      "Average Odds Difference: 0.160704448005745\n",
      "Disparate Impact: 1.6069181278280544\n",
      "Statistical Parity Difference: 0.19568141149116114\n",
      "Equalized Odds Difference: 0.19782046692916522\n",
      "===== Privileged Group Metrics =====\n",
      "TPR:  0.503448275862069\n",
      "FPR:  0.21825396825396826\n",
      "TNR:  0.7817460317460317\n",
      "FNR:  0.496551724137931\n",
      "PPV:  0.5703125\n",
      "===== Unprivileged Group Metrics =====\n",
      "TPR:  0.7012687427912342\n",
      "FPR:  0.341842397336293\n",
      "TNR:  0.658157602663707\n",
      "FNR:  0.29873125720876587\n",
      "PPV:  0.6637554585152838\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate model with sex-based resampling (for females)\n",
    "print(\"\\n=== Sex-Based Resampling Model (Female) ===\")\n",
    "# Create a temporary column for female (where sex_Male = 0)\n",
    "X_train['sex_Female'] = (X_train['sex_Male'] == 0).astype(int)\n",
    "X_train_resampled_sex, y_train_resampled_sex = balanced_resample_by_group(\n",
    "    X_train, y_train_decile, 'sex_Female'\n",
    ")\n",
    "# Remove the temporary column before training\n",
    "X_train_resampled_sex = X_train_resampled_sex.drop('sex_Female', axis=1)\n",
    "\n",
    "sex_model, sex_predictions = train_and_evaluate_model(\n",
    "    X_train_resampled_sex, X_test, y_train_resampled_sex, y_test_decile,\n",
    "    \"Sex-Resampled Model (Female)\"\n",
    ")\n",
    "evaluate_fairness(X_test, y_test_decile, sex_predictions, \"Sex-Resampled Model (Female)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train and evaluate model with sex-based resampling\n",
    "# print(\"\\n=== Sex-Based Resampling Model ===\")\n",
    "# X_train_resampled_sex, y_train_resampled_sex = balanced_resample_by_group(\n",
    "#     X_train, y_train_decile, 'sex_Female'\n",
    "# )\n",
    "# sex_model, sex_predictions = train_and_evaluate_model(\n",
    "#     X_train_resampled_sex, X_test, y_train_resampled_sex, y_test_decile,\n",
    "#     \"Sex-Resampled Model\"\n",
    "# )\n",
    "# evaluate_fairness(X_test, y_test_decile, sex_predictions, \"Sex-Resampled Model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
